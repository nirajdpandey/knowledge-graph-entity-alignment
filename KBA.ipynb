{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "-Cnf45GEK0Ow"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.contrib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-685fbc30e38c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.contrib'"
     ]
    }
   ],
   "source": [
    "from rdflib import Graph, URIRef\n",
    "import random\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import datetime as dt\n",
    "try:\n",
    "    import pickle as cPickle\n",
    "except:\n",
    "    import cPickle\n",
    "    \n",
    "import rdflib\n",
    "import re\n",
    "import collections\n",
    "from tensorflow.contr import rnn\n",
    "\n",
    "import os\n",
    "DEVICE = \"0\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run and see if the code works \n",
    "### Install tensorflow==1.15\n",
    "### save the embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert_dict(d):\n",
    "    return dict([(v, k) for k, v in d.items()])\n",
    "\n",
    "entity_literal_vocab = cPickle.load(open(\"data/vocab_all.pickle\", \"rb\")) # KB1 & KB2 entity and literal vocab\n",
    "char_vocab = cPickle.load(open(\"data/vocab_char.pickle\", \"rb\")) # KB1 & KB2 character vocab\n",
    "entity_vocab = cPickle.load(open(\"data/vocab_entity.pickle\", \"rb\")) # KB1 & KB2 entity vocab\n",
    "predicate_vocab = cPickle.load(open(\"data/vocab_predicate.pickle\", \"rb\")) # KB1 & KB2 predicate vocab\n",
    "entity_kb1_vocab = cPickle.load(open(\"data/vocab_kb1.pickle\", \"rb\")) # KB1 entity vocab for filtering final result\n",
    "entity_kb1_vocab_neg = cPickle.load(open(\"data/vocab_kb1_neg.pickle\", \"rb\")) # KB1 entity & literal vocab for negative sampling\n",
    "entity_kb2_vocab_neg = cPickle.load(open(\"data/vocab_kb2_neg.pickle\", \"rb\")) # KB2 entity & literal vocab for negative sampling\n",
    "entity_label_dict = cPickle.load(open(\"data/entity_label.pickle\", \"rb\")) # KB1 & KB2 entity label\n",
    "entity_literal_kb1_vocab_neg = cPickle.load(open(\"data/vocab_kb1_all_neg.pickle\", \"rb\")) # KB1 entity & literal vocab\n",
    "entity_literal_kb2_vocab_neg = cPickle.load(open(\"data/vocab_kb2_all_neg.pickle\", \"rb\")) # KB1 entity & literal vocab\n",
    "\n",
    "reverse_entity_vocab = invert_dict(entity_vocab)\n",
    "reverse_predicate_vocab = invert_dict(predicate_vocab)\n",
    "reverse_char_vocab = invert_dict(char_vocab)\n",
    "reverse_entity_literal_vocab = invert_dict(entity_literal_vocab)\n",
    "\n",
    "#relationship triples & attribute triples\n",
    "data_uri = cPickle.load(open(\"data/data_uri.pickle\", \"rb\"))\n",
    "data_uri_n = cPickle.load(open(\"data/data_uri_n.pickle\", \"rb\"))\n",
    "data_literal = cPickle.load(open(\"data/data_literal.pickle\", \"rb\"))\n",
    "data_literal_n = cPickle.load(open(\"data/data_literal_n.pickle\", \"rb\"))\n",
    "data_trans = cPickle.load(open(\"data/data_trans.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entity_label_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Methods for data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "_JrNO09iK0PI",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def dataType(string):\n",
    "    odp='string'\n",
    "    patternBIT=re.compile('[01]')\n",
    "    patternINT=re.compile('[0-9]+')\n",
    "    patternFLOAT=re.compile('[0-9]+\\.[0-9]+')\n",
    "    patternTEXT=re.compile('[a-zA-Z0-9]+')\n",
    "    if patternTEXT.match(string):\n",
    "        odp= \"string\"\n",
    "    if patternINT.match(string):\n",
    "        odp= \"integer\"\n",
    "    if patternFLOAT.match(string):\n",
    "        odp= \"float\"\n",
    "    return odp\n",
    "\n",
    "def getRDFData(o):\n",
    "    if isinstance(o, rdflib.term.URIRef):\n",
    "        data_type = \"uri\"\n",
    "    else:\n",
    "        data_type = o.datatype\n",
    "        if data_type == None:\n",
    "            data_type = dataType(o)\n",
    "        else:\n",
    "            if \"#\" in o.datatype:\n",
    "                data_type = o.datatype.split('#')[1].lower()\n",
    "            else:\n",
    "                data_type = dataType(o)\n",
    "        if data_type == 'gmonthday' or data_type=='gyear':\n",
    "            data_type = 'date'\n",
    "        if data_type == 'positiveinteger' or data_type == 'int' or data_type == 'nonnegativeinteger':\n",
    "            data_type = 'integer'\n",
    "    return o, data_type\n",
    "\n",
    "def invert_dict(d):\n",
    "    return dict([(v, k) for k, v in d.items()])\n",
    "\n",
    "def getLiteralArray(o, literal_len, char_vocab):\n",
    "    literal_object = list()\n",
    "    for i in range(literal_len):\n",
    "        literal_object.append(0)\n",
    "    if o[1] != 'uri':\n",
    "        max_len = min(literal_len, len(o[0]))\n",
    "        for i in range(max_len):\n",
    "            if char_vocab.get(o[0][i]) == None:\n",
    "                char_vocab[o[0][i]] = len(char_vocab)\n",
    "            literal_object[i] = char_vocab[o[0][i]]\n",
    "    elif entity_label_dict.get(o[0]) != None:\n",
    "        label = entity_label_dict.get(o[0])\n",
    "        max_len = min(literal_len, len(label))\n",
    "        for i in range(max_len):\n",
    "            if char_vocab.get(label[i]) == None:\n",
    "                char_vocab[label[i]] = len(char_vocab)\n",
    "            literal_object[i] = char_vocab[label[i]]\n",
    "    return literal_object\n",
    "\n",
    "def getBatch(data, batchSize, current, entityVocab, literal_len, char_vocab):\n",
    "    hasNext = current+batchSize < len(data)\n",
    "    \n",
    "    if (len(data) - current) < batchSize:\n",
    "        current = current - (batchSize - (len(data) - current))\n",
    "    \n",
    "    dataPos_all = data[current:current+batchSize]\n",
    "    dataPos = list()\n",
    "    charPos = list()\n",
    "    pred_weight_pos = list()\n",
    "    dataNeg = list()\n",
    "    charNeg = list()\n",
    "    pred_weight_neg = list()\n",
    "    for triples,chars, pred_weight in dataPos_all:\n",
    "        s,p,o,p_trans = triples\n",
    "        dataPos.append([s,p,o,p_trans])\n",
    "        charPos.append(chars)\n",
    "        pred_weight_pos.append(pred_weight)\n",
    "        lr = round(random.random())\n",
    "        \n",
    "        if lr == 0:\n",
    "            try:\n",
    "                o_type = getRDFData(reverse_entity_vocab[o])\n",
    "            except:\n",
    "                o_type = 'not_uri'\n",
    "            \n",
    "            literal_array = []\n",
    "            rerun = True\n",
    "            while rerun or negElm[0] == (reverse_entity_vocab[o] and literal_array == chars):\n",
    "                if o_type[1] == 'uri':\n",
    "                    if str(s).startswith('http://dbpedia.org/resource/'):\n",
    "                        negElm = entity_kb1_vocab_neg[random.randint(0, len(entity_kb1_vocab_neg)-1)]\n",
    "                        negElm = reverse_entity_vocab[entity_vocab[negElm]]\n",
    "                    else:\n",
    "                        negElm = entity_kb2_vocab_neg[random.randint(0, len(entity_kb2_vocab_neg)-1)]\n",
    "                        negElm = reverse_entity_vocab[entity_vocab[negElm]]\n",
    "                else:\n",
    "                    if str(s).startswith('http://dbpedia.org/resource/'):\n",
    "                        negElm = entity_literal_kb1_vocab_neg[random.randint(0, len(entity_literal_kb1_vocab_neg)-1)]\n",
    "                        negElm = reverse_entity_literal_vocab[entity_literal_vocab[negElm]]\n",
    "                    else:\n",
    "                        negElm = entity_literal_kb2_vocab_neg[random.randint(0, len(entity_literal_kb2_vocab_neg)-1)]\n",
    "                        negElm = reverse_entity_literal_vocab[entity_literal_vocab[negElm]]\n",
    "                if o_type == 'uri' and negElm[1] == 'uri':\n",
    "                    rerun = False\n",
    "                elif o_type != 'uri':\n",
    "                    rerun = False\n",
    "                if (isinstance(negElm, rdflib.term.URIRef)) or (isinstance(negElm, rdflib.term.Literal)):\n",
    "                    negElm = getRDFData(negElm)\n",
    "                    literal_array = getLiteralArray(negElm, literal_len, char_vocab)\n",
    "                else:\n",
    "                    rerun = True    \n",
    "            if negElm[1] == 'uri':\n",
    "                dataNeg.append([s, p, entity_vocab[negElm[0]], p_trans])\n",
    "            else:\n",
    "                dataNeg.append([s, p, entity_vocab[negElm[1]], p_trans])\n",
    "            charNeg.append(literal_array)\n",
    "            pred_weight_neg.append(pred_weight)\n",
    "        else:\n",
    "            negElm = random.randint(0, len(entity_vocab)-1)\n",
    "            while negElm == s:\n",
    "                negElm = random.randint(0, len(entity_vocab)-1)\n",
    "            dataNeg.append([negElm, p, o, p_trans])\n",
    "            charNeg.append(chars)\n",
    "            pred_weight_neg.append(pred_weight)\n",
    "            \n",
    "    dataPos = np.array(dataPos)\n",
    "    charPos = np.array(charPos)\n",
    "    pred_weight_pos = np.array(pred_weight_pos)\n",
    "    dataNeg = np.array(dataNeg)\n",
    "    charNeg = np.array(charNeg)\n",
    "    pred_weight_neg = np.array(pred_weight_neg)\n",
    "    return hasNext, current+batchSize, dataPos[:,0], dataPos[:,1], dataPos[:,2], dataPos[:,3], pred_weight_pos, charPos, dataNeg[:,0], dataNeg[:,1], dataNeg[:,2], dataNeg[:,3], pred_weight_neg, charNeg "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "HpyMnQCsK0PL",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "batchSize = 100\n",
    "hidden_size = 100\n",
    "totalEpoch = 1\n",
    "verbose = 1000\n",
    "margin = 1.0\n",
    "literal_len = 10\n",
    "entitySize = len(entity_vocab)\n",
    "predSize = len(predicate_vocab)\n",
    "charSize = len(char_vocab)\n",
    "top_k = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "M8DdUZv5K0PP",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from rdflib import URIRef\n",
    "\n",
    "file_mapping = open(\"data/mapping.ttl\", 'r')\n",
    "\n",
    "test_dataset_list = list()\n",
    "for line in file_mapping:\n",
    "    elements = line.split(' ')\n",
    "    s = elements[0]\n",
    "    p = elements[1]\n",
    "    o = elements[2]\n",
    "    \n",
    "    if (entity_vocab[URIRef(s.replace('<','').replace('>',''))] in entity_kb1_vocab) and (URIRef(o.replace('<','').replace('>','')) in entity_vocab):\n",
    "        test_dataset_list.append((o, s))\n",
    "file_mapping.close()\n",
    "\n",
    "test_input = [entity_vocab[URIRef(k.replace('<','').replace('>',''))] for k,_ in test_dataset_list]\n",
    "test_answer = [entity_kb1_vocab.index(entity_vocab[URIRef(k.replace('<','').replace('>',''))]) for _,k in test_dataset_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4950,
     "status": "ok",
     "timestamp": 1532297359735,
     "user": {
      "displayName": "Bayu Distiawan",
      "photoUrl": "//lh5.googleusercontent.com/-ZlEtcF5Few0/AAAAAAAAAAI/AAAAAAAAAAA/caTzimkqvdg/s50-c-k-no/photo.jpg",
      "userId": "113711532389286602961"
     },
     "user_tz": 0
    },
    "id": "PHGBLgFlK0PU",
    "outputId": "4c34477f-958b-4d07-c7d2-c60b1901c4b5"
   },
   "outputs": [],
   "source": [
    "tfgraph = tf.Graph()\n",
    "\n",
    "with tfgraph.as_default():\n",
    "    pos_h = tf.placeholder(tf.int32, [None])\n",
    "    pos_t = tf.placeholder(tf.int32, [None])\n",
    "    pos_r = tf.placeholder(tf.int32, [None])\n",
    "    pos_r_trans = tf.placeholder(tf.int32, [None])\n",
    "    pos_c = tf.placeholder(tf.int32, [None, literal_len])\n",
    "    pos_pred_weight = tf.placeholder(tf.float32, [None,1], name='pos_pred_weight')\n",
    "\n",
    "    neg_h = tf.placeholder(tf.int32, [None])\n",
    "    neg_t = tf.placeholder(tf.int32, [None])\n",
    "    neg_r = tf.placeholder(tf.int32, [None])\n",
    "    neg_r_trans = tf.placeholder(tf.int32, [None])\n",
    "    neg_c = tf.placeholder(tf.int32, [None, literal_len])\n",
    "    neg_pred_weight = tf.placeholder(tf.float32, [None,1], name='neg_pred_weight')\n",
    "    \n",
    "    type_data = tf.placeholder(tf.int32, [1])\n",
    "    type_trans = tf.placeholder(tf.int32, [1])\n",
    "    \n",
    "    ent_embeddings_ori = tf.get_variable(name = \"relationship_ent_embedding\", shape = [entitySize, hidden_size], initializer = tf.contrib.layers.xavier_initializer(uniform = False))\n",
    "    atr_embeddings_ori = tf.get_variable(name = \"attribute_ent_embedding\", shape = [entitySize, hidden_size], initializer = tf.contrib.layers.xavier_initializer(uniform = False))\n",
    "    rel_embeddings = tf.get_variable(name = \"rel_embedding\", shape = [predSize, hidden_size], initializer = tf.contrib.layers.xavier_initializer(uniform = False))\n",
    "    attribute_rel_embeddings = tf.get_variable(name = \"attribute_rel_embedding\", shape = [predSize, hidden_size], initializer = tf.contrib.layers.xavier_initializer(uniform = False))\n",
    "    char_embeddings = tf.get_variable(name = \"attribute_char_embedding\", shape = [charSize, hidden_size], initializer = tf.contrib.layers.xavier_initializer(uniform = False))\n",
    "    \n",
    "    ent_indices = tf.concat([pos_h, pos_t, neg_h, neg_t], 0)\n",
    "    ent_indices = tf.reshape(ent_indices,[-1,1])\n",
    "    ent_value = tf.concat([tf.nn.embedding_lookup(ent_embeddings_ori, pos_h),\\\n",
    "                          tf.nn.embedding_lookup(ent_embeddings_ori, pos_t),\\\n",
    "                          tf.nn.embedding_lookup(ent_embeddings_ori, neg_h),\\\n",
    "                          tf.nn.embedding_lookup(ent_embeddings_ori, neg_t)], 0)\n",
    "    part_ent_embeddings = tf.scatter_nd([ent_indices], [ent_value], ent_embeddings_ori.shape)\n",
    "    ent_embeddings = part_ent_embeddings + tf.stop_gradient(-part_ent_embeddings + ent_embeddings_ori)\n",
    "    \n",
    "    atr_indices = tf.concat([pos_h, pos_t, neg_h, neg_t], 0)\n",
    "    atr_indices = tf.reshape(atr_indices,[-1,1])\n",
    "    atr_value = tf.concat([tf.nn.embedding_lookup(atr_embeddings_ori, pos_h),\\\n",
    "                          tf.nn.embedding_lookup(atr_embeddings_ori, pos_t),\\\n",
    "                          tf.nn.embedding_lookup(atr_embeddings_ori, neg_h),\\\n",
    "                          tf.nn.embedding_lookup(atr_embeddings_ori, neg_t)], 0)\n",
    "    part_atr_embeddings = tf.scatter_nd([atr_indices], [atr_value], atr_embeddings_ori.shape)\n",
    "    atr_embeddings = part_atr_embeddings + tf.stop_gradient(-part_atr_embeddings + atr_embeddings_ori)\n",
    "\n",
    "    pos_h_e = tf.cond(type_data[0] > 0, lambda: tf.nn.embedding_lookup(ent_embeddings, pos_h), lambda: tf.nn.embedding_lookup(atr_embeddings, pos_h))\n",
    "    pos_t_e = tf.cond(type_data[0] > 0, lambda: tf.stop_gradient(tf.nn.embedding_lookup(ent_embeddings, pos_t)), lambda: tf.nn.embedding_lookup(atr_embeddings, pos_t))\n",
    "    pos_r_e = tf.cond(type_data[0] > 0, lambda: tf.nn.embedding_lookup(rel_embeddings, pos_r), lambda: tf.nn.embedding_lookup(attribute_rel_embeddings, pos_r))\n",
    "    pos_r_e_trans = tf.nn.embedding_lookup(rel_embeddings, pos_r_trans)\n",
    "    pos_c_e = tf.nn.embedding_lookup(char_embeddings, pos_c)\n",
    "    neg_h_e = tf.cond(type_data[0] > 0, lambda: tf.nn.embedding_lookup(ent_embeddings, neg_h), lambda: tf.nn.embedding_lookup(atr_embeddings, neg_h))\n",
    "    neg_t_e = tf.cond(type_data[0] > 0, lambda: tf.stop_gradient(tf.nn.embedding_lookup(ent_embeddings, neg_t)), lambda: tf.nn.embedding_lookup(atr_embeddings, neg_t))\n",
    "    neg_r_e = tf.cond(type_data[0] > 0, lambda: tf.nn.embedding_lookup(rel_embeddings, neg_r), lambda: tf.nn.embedding_lookup(attribute_rel_embeddings, neg_r))\n",
    "    neg_r_e_trans = tf.nn.embedding_lookup(rel_embeddings, neg_r_trans)\n",
    "    neg_c_e = tf.nn.embedding_lookup(char_embeddings, neg_c)\n",
    "    \n",
    "    pos_r_e = tf.cond(type_trans[0] < 1, lambda: pos_r_e, lambda: tf.multiply(pos_r_e, pos_r_e_trans))\n",
    "    neg_r_e = tf.cond(type_trans[0] < 1, lambda: neg_r_e, lambda: tf.multiply(neg_r_e, neg_r_e_trans))\n",
    "    \n",
    "    mask_constant_0 = np.zeros([1,hidden_size])\n",
    "    mask_constant_1 = np.ones([1,hidden_size])\n",
    "    mask_constant = np.concatenate([mask_constant_0, mask_constant_1])\n",
    "    mask_constant = tf.constant(mask_constant, tf.float32)\n",
    "    \n",
    "    flag_pos_c_e = tf.sign(tf.abs(pos_c))\n",
    "    mask_pos_c_e = tf.nn.embedding_lookup(mask_constant, flag_pos_c_e)\n",
    "    pos_c_e = pos_c_e * mask_pos_c_e\n",
    "    \n",
    "    flag_neg_c_e = tf.sign(tf.abs(neg_c))\n",
    "    mask_neg_c_e = tf.nn.embedding_lookup(mask_constant, flag_neg_c_e)\n",
    "    neg_c_e = neg_c_e * mask_neg_c_e\n",
    "    \n",
    "    def calculate_ngram_weight(unstacked_tensor):\n",
    "        stacked_tensor = tf.stack(unstacked_tensor, 1)\n",
    "        stacked_tensor = tf.reverse(stacked_tensor, [1])\n",
    "        index = tf.constant(len(unstacked_tensor))\n",
    "        expected_result = tf.zeros([batchSize, hidden_size])\n",
    "        def condition(index, summation):\n",
    "            return tf.greater(index, 0)\n",
    "        def body(index, summation):\n",
    "            precessed = tf.slice(stacked_tensor,[0,index-1,0], [-1,-1,-1])\n",
    "            summand = tf.reduce_mean(precessed, 1)\n",
    "            return tf.subtract(index, 1), tf.add(summation, summand)\n",
    "        result = tf.while_loop(condition, body, [index, expected_result])\n",
    "        return result[1]\n",
    "    \n",
    "    pos_c_e_in_lstm = tf.unstack(pos_c_e, literal_len, 1)\n",
    "    pos_c_e_lstm = calculate_ngram_weight(pos_c_e_in_lstm)\n",
    "    \n",
    "    neg_c_e_in_lstm = tf.unstack(neg_c_e, literal_len, 1)\n",
    "    neg_c_e_lstm = calculate_ngram_weight(neg_c_e_in_lstm)\n",
    "    \n",
    "    tail_pos = tf.cond(type_data[0] > 0, lambda: pos_t_e, lambda: pos_c_e_lstm)\n",
    "    tail_neg = tf.cond(type_data[0] > 0, lambda: neg_t_e, lambda: neg_c_e_lstm)\n",
    "    \n",
    "    pos = tf.reduce_sum(abs(pos_h_e + pos_r_e - tail_pos), 1, keep_dims = True)\n",
    "    neg = tf.reduce_sum(abs(neg_h_e + neg_r_e - tail_neg), 1, keep_dims = True)\n",
    "    \n",
    "    pos = tf.cond(type_data[0] > 0, lambda: pos, lambda: tf.multiply(pos, pos_pred_weight))\n",
    "    neg = tf.cond(type_data[0] > 0, lambda: neg, lambda: tf.multiply(neg, neg_pred_weight))\n",
    "    learning_rate = tf.cond(type_data[0] > 0, lambda: 0.01, lambda: tf.reduce_min(pos_pred_weight)*0.01)\n",
    "    \n",
    "    opt_vars_ent = [v for v in tf.trainable_variables() if v.name.startswith(\"relationship\") or v.name.startswith(\"rel_embedding\")]\n",
    "    opt_vars_atr = [v for v in tf.trainable_variables() if v.name.startswith(\"attribute\") or v.name.startswith(\"attribute_rel_embedding\") or v.name.startswith(\"rnn\")]\n",
    "    opt_vars_sim = [v for v in tf.trainable_variables() if v.name.startswith(\"relationship_ent_embedding\") or v.name.startswith(\"attribute_rel_embedding\")]\n",
    "    opt_vars = [v for v in tf.trainable_variables()]\n",
    "    \n",
    "    ent_emb = tf.cond(type_data[0] > 0, lambda: tf.nn.embedding_lookup(ent_embeddings, pos_t), lambda: tf.nn.embedding_lookup(ent_embeddings, pos_h))\n",
    "    atr_emb = tf.cond(type_data[0] > 0, lambda: tf.nn.embedding_lookup(atr_embeddings, pos_t), lambda: tf.nn.embedding_lookup(atr_embeddings, pos_h))\n",
    "    norm_ent_emb = tf.nn.l2_normalize(ent_emb,1)\n",
    "    norm_atr_emb = tf.nn.l2_normalize(atr_emb,1)\n",
    "    cos_sim = tf.reduce_sum(tf.multiply(norm_ent_emb, norm_atr_emb), 1, keep_dims=True)\n",
    "    sim_loss = tf.reduce_sum(1-cos_sim)\n",
    "    sim_optimizer = tf.train.AdamOptimizer(0.01).minimize(sim_loss, var_list=opt_vars_sim)\n",
    "    \n",
    "    loss = tf.cond(type_data[0] > 0, lambda: tf.reduce_sum(tf.maximum(pos - neg + 1, 0) + (1-cos_sim)), lambda: tf.reduce_sum(tf.maximum(pos - neg + 1, 0)))\n",
    "    loss = tf.cond(type_trans[0] < 1, lambda: loss, lambda: tf.multiply(loss, 0.1))\n",
    "    optimizer = tf.cond(type_data[0] > 0, lambda: tf.train.AdamOptimizer(learning_rate).minimize(loss, var_list=opt_vars_ent), lambda: tf.train.AdamOptimizer(learning_rate).minimize(loss, var_list=opt_vars_atr))\n",
    "    \n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(ent_embeddings_ori), 1, keep_dims=True))\n",
    "    normalized_embeddings = ent_embeddings_ori / norm\n",
    "    \n",
    "    test_dataset = tf.constant(test_input, dtype=tf.int32)\n",
    "    test_embeddings = tf.nn.embedding_lookup(normalized_embeddings, test_dataset)\n",
    "    similarity = tf.matmul(test_embeddings, normalized_embeddings, transpose_b=True)\n",
    "    \n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "5Se9GHctK0PZ"
   },
   "outputs": [],
   "source": [
    "def metric(y_true, y_pred, answer_vocab, k=10):\n",
    "    list_rank = list()\n",
    "    total_hits = 0\n",
    "    total_hits_1 = 0\n",
    "    for i in range(len(y_true)):\n",
    "        result = y_pred[i]\n",
    "        result = result[answer_vocab]\n",
    "        result = (-result).argsort()\n",
    "        \n",
    "        for j in range(len(result)):\n",
    "            if result[j] == y_true[i]:\n",
    "                rank = j\n",
    "                break\n",
    "        list_rank.append(j)\n",
    "        \n",
    "        result = result[:k]\n",
    "        for j in range(len(result)):\n",
    "            if result[j] == y_true[i]:\n",
    "                total_hits += 1\n",
    "                if j == 0:\n",
    "                    total_hits_1 += 1\n",
    "                break    \n",
    "    return reduce(lambda x, y: x + y, list_rank) / len(list_rank), float(total_hits)/len(y_true), float(total_hits_1)/len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "5mbanxArK0Pd"
   },
   "outputs": [],
   "source": [
    "def run(graph, totalEpoch):\n",
    "    writer = open('log.txt', 'w', 0)\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        init.run()\n",
    "        \n",
    "        for epoch in range(totalEpoch):\n",
    "            if epoch % 2 == 0:\n",
    "                data = [data_uri_n, data_uri, data_literal_n, data_literal,[], data_trans]\n",
    "            else:\n",
    "                data = [[],[],data_literal_n,data_literal,[],data_trans]\n",
    "            start_time_epoch = dt.datetime.now()\n",
    "            for i in range(0, len(data)):\n",
    "                random.shuffle(data[i])\n",
    "                hasNext = True\n",
    "                current = 0\n",
    "                step = 0\n",
    "                average_loss = 0\n",
    "                \n",
    "                if i > 3:\n",
    "                    transitive = 1\n",
    "                else:\n",
    "                    transitive = 0\n",
    "                \n",
    "                if i == 0 or i ==1 or i == 4:\n",
    "                    uri = 1\n",
    "                else:\n",
    "                    uri = 0\n",
    "                  \n",
    "                while(hasNext and len(data[i]) > 0):\n",
    "                    step += 1\n",
    "                    hasNext, current, ph, pr, pt, pr_trans, ppred, pc, nh, nr, nt, nr_trans, npred, nc = getBatch(data[i], batchSize, current, entity_vocab, literal_len, char_vocab)\n",
    "                    feed_dict = {\n",
    "                        pos_h: ph,\n",
    "                        pos_t: pt,\n",
    "                        pos_r: pr,\n",
    "                        pos_r_trans: pr_trans,\n",
    "                        pos_pred_weight : ppred,\n",
    "                        pos_c: pc,\n",
    "                        neg_h: nh,\n",
    "                        neg_t: nt,\n",
    "                        neg_r: nr,\n",
    "                        neg_r_trans: nr_trans,\n",
    "                        neg_c: nc,\n",
    "                        neg_pred_weight: npred,\n",
    "                        type_data : np.full([1],uri),\n",
    "                        type_trans : np.full([1],transitive)\n",
    "                    }\n",
    "                    if epoch % 2 == 0:\n",
    "                        __, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "                        average_loss += loss_val\n",
    "                    else:\n",
    "                        __, loss_val = session.run([sim_optimizer, sim_loss], feed_dict=feed_dict)\n",
    "                        average_loss += loss_val\n",
    "\n",
    "                    if step % verbose == 0:\n",
    "                        average_loss /= verbose\n",
    "                        print('Epoch: ', epoch, ' Average loss at step ', step, ': ', average_loss)\n",
    "                        writer.write('Epoch: '+ str(epoch)+ ' Average loss at step '+ str(step)+ ': '+ str(average_loss)+'\\n')\n",
    "                        average_loss = 0\n",
    "                if len(data[i]) > 0:\n",
    "                        average_loss /= ((len(data[i])%(verbose*batchSize))/batchSize)\n",
    "                        print('Epoch: ', epoch, ' Average loss at step ', step, ': ', average_loss)\n",
    "                        writer.write('Epoch: '+ str(epoch)+ ' Average loss at step '+ str(step)+ ': '+ str(average_loss)+ '\\n')\n",
    "\n",
    "            end_time_epoch = dt.datetime.now()\n",
    "            print(\"Training time took {} seconds to run 1 epoch\".format((end_time_epoch-start_time_epoch).total_seconds()))\n",
    "            writer.write(\"Training time took {} seconds to run 1 epoch\\n\".format((end_time_epoch-start_time_epoch).total_seconds()))\n",
    "            if (epoch+1) % 10 == 0:\n",
    "                start_time_epoch = dt.datetime.now()\n",
    "                sim = similarity.eval()\n",
    "                mean_rank, hits_at_10, hits_at_1 = metric(test_answer, sim, entity_kb1_vocab, top_k)\n",
    "                print \"Mean Rank: \", mean_rank, \" of \", len(entity_kb1_vocab)\n",
    "                writer.write(\"Mean Rank: \"+ str(mean_rank)+ \" of \"+ str(len(entity_kb1_vocab))+ \"\\n\")\n",
    "                print \"Hits @ \"+str(top_k)+\": \", hits_at_10\n",
    "                writer.write(\"Hits @ \"+str(top_k)+\": \"+ str(hits_at_10)+ \"\\n\")\n",
    "                print \"Hits @ \"+str(1)+\": \", hits_at_1\n",
    "                writer.write(\"Hits @ \"+str(1)+\": \"+ str(hits_at_1)+ \"\\n\")\n",
    "                end_time_epoch = dt.datetime.now()\n",
    "                print(\"Testing time took {} seconds.\".format((end_time_epoch-start_time_epoch).total_seconds()))\n",
    "                writer.write(\"Testing time took {} seconds.\\n\\n\".format((end_time_epoch-start_time_epoch).total_seconds()))\n",
    "                print\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "HejbGJdIK0Pf"
   },
   "outputs": [],
   "source": [
    "start_time = dt.datetime.now()\n",
    "run(tfgraph, totalEpoch) \n",
    "end_time = dt.datetime.now()\n",
    "print(\"Training time took {} seconds to run {} epoch\".format((end_time-start_time).total_seconds(), totalEpoch))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "TextKE6(TRANS).ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
